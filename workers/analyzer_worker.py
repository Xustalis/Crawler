"""
Refactored Crawler Worker with smart URL handling and aggregated results.

Focuses on user experience:
- Smart URL normalization with HTTPS/HTTP fallback
- Aggregated results by category (no raw URL lists shown to users)
- Clean signal-based communication
"""

from typing import Optional, Set
from urllib.parse import urljoin, urlparse
import re

from PyQt6.QtCore import QThread, pyqtSignal, QObject
from bs4 import BeautifulSoup
from requests.exceptions import RequestException

from core.scraped_data import ScrapedData, ResourceCategory, CATEGORY_DISPLAY
from utils.url_normalizer import normalize_url, fetch_with_fallback
from utils.logger import setup_logger

logger = setup_logger(__name__)


class AnalyzerSignals(QObject):
    """Signals for the analyzer worker."""
    
    # åˆ†æå¼€å§‹
    started = pyqtSignal()
    
    # åˆ†æè¿›åº¦ (é˜¶æ®µæè¿°)
    progress = pyqtSignal(str)
    
    # åˆ†æå®Œæˆï¼Œè¿”å›èšåˆç»“æœ
    finished = pyqtSignal(ScrapedData)
    
    # å‘ç”Ÿé”™è¯¯
    error = pyqtSignal(str)
    
    # æ—¥å¿—æ¶ˆæ¯
    log = pyqtSignal(str)


class AnalyzerWorker(QThread):
    """
    Analyzer Worker Thread.
    
    Responsibilities:
    1. Fetch webpage content (with HTTPS->HTTP fallback)
    2. Parse HTML and extract resources
    3. Categorize resources into ScrapedData
    4. Emit aggregated results to UI
    
    This worker ONLY does analysis. Download is handled separately.
    """
    
    # èµ„æºæ–‡ä»¶æ‰©å±•åæ˜ å°„
    IMAGE_EXTENSIONS = {'.jpg', '.jpeg', '.png', '.gif', '.webp', '.bmp', '.svg', '.ico', '.avif'}
    VIDEO_EXTENSIONS = {'.mp4', '.webm', '.avi', '.mov', '.mkv', '.flv', '.wmv'}
    AUDIO_EXTENSIONS = {'.mp3', '.wav', '.ogg', '.flac', '.aac', '.m4a', '.wma'}
    DOCUMENT_EXTENSIONS = {'.pdf', '.doc', '.docx', '.xls', '.xlsx', '.ppt', '.pptx', '.txt'}
    
    def __init__(self, url: str, parent=None):
        super().__init__(parent)
        self.url = url
        self.signals = AnalyzerSignals()
        self._is_cancelled = False
    
    def run(self) -> None:
        """Execute the analysis task."""
        try:
            self.signals.started.emit()
            
            # Step 1: è§„èŒƒåŒ– URL
            self.signals.progress.emit("æ­£åœ¨è§„èŒƒåŒ– URL...")
            normalized_url = normalize_url(self.url)
            
            if not normalized_url:
                self.signals.error.emit("è¯·è¾“å…¥æœ‰æ•ˆçš„ç½‘å€")
                return
            
            self.signals.log.emit(f"ğŸ“¡ æ­£åœ¨åˆ†æ: {normalized_url}")
            
            # Step 2: è·å–ç½‘é¡µå†…å®¹ (å¸¦ HTTPS->HTTP é™çº§)
            self.signals.progress.emit("æ­£åœ¨è·å–ç½‘é¡µå†…å®¹...")
            
            try:
                response, final_url = fetch_with_fallback(normalized_url)
            except RequestException as e:
                self.signals.error.emit(str(e))
                return
            
            if self._is_cancelled:
                return
            
            self.signals.log.emit(f"âœ“ æˆåŠŸè·å–ç½‘é¡µ ({len(response.content)} bytes)")
            
            # Step 3: è§£æ HTML
            self.signals.progress.emit("æ­£åœ¨è§£æç½‘é¡µå†…å®¹...")
            soup = BeautifulSoup(response.text, 'lxml')
            
            # Step 4: æå–å¹¶åˆ†ç±»èµ„æº
            self.signals.progress.emit("æ­£åœ¨åˆ†ç±»èµ„æº...")
            scraped_data = self._extract_and_categorize(soup, final_url)
            scraped_data.source_url = final_url
            
            if self._is_cancelled:
                return
            
            # Step 5: å‘é€èšåˆç»“æœ
            self.signals.log.emit(f"âœ“ åˆ†æå®Œæˆ: {scraped_data.summary()}")
            self.signals.finished.emit(scraped_data)
            
        except Exception as e:
            logger.exception("Analyzer worker error")
            self.signals.error.emit(f"åˆ†æå¤±è´¥: {str(e)}")
    
    def _extract_and_categorize(self, soup: BeautifulSoup, base_url: str) -> ScrapedData:
        """
        Extract resources from HTML and categorize them.
        
        éµå¾ªä¼˜å…ˆçº§ï¼šè§†é¢‘ > M3U8 > å›¾ç‰‡ > éŸ³é¢‘ > æ–‡æ¡£
        å»é‡å¤„ç†ï¼Œé¿å…é‡å¤ URL
        """
        scraped = ScrapedData()
        seen_urls: Set[str] = set()
        
        # 1. æå– <img> æ ‡ç­¾ä¸­çš„å›¾ç‰‡
        for img in soup.find_all('img'):
            src = img.get('src') or img.get('data-src') or img.get('data-lazy-src')
            if src:
                url = self._resolve_url(src, base_url)
                if url and url not in seen_urls:
                    if self._has_extension(url, self.IMAGE_EXTENSIONS):
                        scraped.images.append(url)
                        seen_urls.add(url)
        
        # 2. æå– <video> å’Œ <source> æ ‡ç­¾ä¸­çš„è§†é¢‘
        for video in soup.find_all(['video', 'source']):
            src = video.get('src')
            if src:
                url = self._resolve_url(src, base_url)
                if url and url not in seen_urls:
                    # æ£€æŸ¥æ˜¯å¦ä¸º M3U8 æµ
                    if '.m3u8' in url.lower() or '.m3u' in url.lower():
                        scraped.m3u8_streams.append(url)
                    elif self._has_extension(url, self.VIDEO_EXTENSIONS):
                        scraped.videos.append(url)
                    seen_urls.add(url)
        
        # 3. ä» <script> å’Œæ–‡æœ¬ä¸­æ£€æµ‹ M3U8 é“¾æ¥
        page_text = str(soup)
        m3u8_pattern = r'https?://[^\s\'"<>]+\.m3u8[^\s\'"<>]*'
        for match in re.findall(m3u8_pattern, page_text, re.IGNORECASE):
            url = match.rstrip('\\').rstrip('"').rstrip("'")
            if url not in seen_urls:
                scraped.m3u8_streams.append(url)
                seen_urls.add(url)
        
        # 4. æå– <audio> æ ‡ç­¾ä¸­çš„éŸ³é¢‘
        for audio in soup.find_all(['audio', 'source']):
            src = audio.get('src')
            if src:
                url = self._resolve_url(src, base_url)
                if url and url not in seen_urls:
                    if self._has_extension(url, self.AUDIO_EXTENSIONS):
                        scraped.audios.append(url)
                        seen_urls.add(url)
        
        # 5. æå– <a> é“¾æ¥ä¸­çš„èµ„æº
        for link in soup.find_all('a', href=True):
            href = link['href']
            url = self._resolve_url(href, base_url)
            if url and url not in seen_urls:
                # åˆ†ç±»é“¾æ¥ç±»å‹
                if self._has_extension(url, self.IMAGE_EXTENSIONS):
                    scraped.images.append(url)
                elif self._has_extension(url, self.VIDEO_EXTENSIONS):
                    scraped.videos.append(url)
                elif self._has_extension(url, self.AUDIO_EXTENSIONS):
                    scraped.audios.append(url)
                elif self._has_extension(url, self.DOCUMENT_EXTENSIONS):
                    scraped.documents.append(url)
                elif '.m3u8' in url.lower():
                    scraped.m3u8_streams.append(url)
                else:
                    continue  # è·³è¿‡æ™®é€šé“¾æ¥
                seen_urls.add(url)
        
        logger.info(
            f"Extracted: {len(scraped.images)} images, "
            f"{len(scraped.videos)} videos, "
            f"{len(scraped.m3u8_streams)} M3U8 streams"
        )
        
        return scraped
    
    def _resolve_url(self, url: str, base_url: str) -> Optional[str]:
        """Resolve relative URL to absolute."""
        if not url:
            return None
        
        url = url.strip()
        
        # è·³è¿‡ data URI å’Œ javascript
        if url.startswith(('data:', 'javascript:', '#', 'mailto:')):
            return None
        
        # è½¬æ¢ä¸ºç»å¯¹ URL
        try:
            resolved = urljoin(base_url, url)
            # éªŒè¯æ˜¯æœ‰æ•ˆçš„ HTTP(S) URL
            parsed = urlparse(resolved)
            if parsed.scheme in ('http', 'https') and parsed.netloc:
                return resolved
        except Exception:
            pass
        
        return None
    
    def _has_extension(self, url: str, extensions: Set[str]) -> bool:
        """Check if URL has one of the specified extensions."""
        try:
            parsed = urlparse(url)
            path = parsed.path.lower()
            return any(path.endswith(ext) for ext in extensions)
        except Exception:
            return False
    
    def cancel(self) -> None:
        """Cancel the analysis."""
        self._is_cancelled = True
